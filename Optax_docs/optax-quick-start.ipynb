{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import optax\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "NUM_TRAIN_STEPS = 1_000\n",
    "RAW_TRAINING_DATA = np.random.randint(255, size=(NUM_TRAIN_STEPS, BATCH_SIZE, 1))\n",
    "\n",
    "TRAINING_DATA = np.unpackbits(RAW_TRAINING_DATA.astype(np.uint8), axis=-1)\n",
    "LABELS = jax.nn.one_hot(RAW_TRAINING_DATA % 2, 2).astype(jnp.float32).reshape(NUM_TRAIN_STEPS, BATCH_SIZE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = {\n",
    "    'hidden': jax.random.normal(shape=[8, 32], key=jax.random.PRNGKey(0)),\n",
    "    'output': jax.random.normal(shape=[32, 2], key=jax.random.PRNGKey(1)),\n",
    "}\n",
    "\n",
    "\n",
    "def net(x: jnp.ndarray, params: jnp.ndarray) -> jnp.ndarray:\n",
    "  x = jnp.dot(x, params['hidden'])\n",
    "  x = jax.nn.relu(x)\n",
    "  x = jnp.dot(x, params['output'])\n",
    "  return x\n",
    "\n",
    "\n",
    "def loss(params: optax.Params, batch: jnp.ndarray, labels: jnp.ndarray) -> jnp.ndarray:\n",
    "  y_hat = net(batch, params)\n",
    "\n",
    "  # optax also provides a number of common loss functions.\n",
    "  loss_value = optax.sigmoid_binary_cross_entropy(y_hat, labels).sum(axis=-1)\n",
    "\n",
    "  return loss_value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 5.507180690765381\n",
      "step 100, loss: 0.229142427444458\n",
      "step 200, loss: 0.03447369486093521\n",
      "step 300, loss: 0.006939497776329517\n",
      "step 400, loss: 0.006980330683290958\n",
      "step 500, loss: 0.015318550169467926\n",
      "step 600, loss: 0.009581432677805424\n",
      "step 700, loss: 0.00931396521627903\n",
      "step 800, loss: 0.004092650953680277\n",
      "step 900, loss: 0.0008034102502278984\n"
     ]
    }
   ],
   "source": [
    "def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "  opt_state = optimizer.init(params)\n",
    "\n",
    "  @jax.jit\n",
    "  def step(params, opt_state, batch, labels):\n",
    "    loss_value, grads = jax.value_and_grad(loss)(params, batch, labels)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss_value\n",
    "\n",
    "  for i, (batch, labels) in enumerate(zip(TRAINING_DATA, LABELS)):\n",
    "    params, opt_state, loss_value = step(params, opt_state, batch, labels)\n",
    "    if i % 100 == 0:\n",
    "      print(f'step {i}, loss: {loss_value}')\n",
    "\n",
    "  return params\n",
    "\n",
    "# Finally, we can fit our parametrized function using the Adam optimizer\n",
    "# provided by optax.\n",
    "optimizer = optax.adam(learning_rate=1e-2)\n",
    "params = fit(initial_params, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 5.507180690765381\n",
      "step 100, loss: 1.0336355463053415e-17\n",
      "step 200, loss: 3.111113797138465e-12\n",
      "step 300, loss: 6.953975722579785e-17\n",
      "step 400, loss: 1.3526211438856197e-18\n",
      "step 500, loss: 1.2548147416968636e-11\n",
      "step 600, loss: 6.068839875084109e-11\n",
      "step 700, loss: 1.343288147381827e-07\n",
      "step 800, loss: 1.6410630195923996e-15\n",
      "step 900, loss: 1.357956165293217e-07\n"
     ]
    }
   ],
   "source": [
    "schedule = optax.warmup_cosine_decay_schedule(\n",
    "  init_value=0.0,\n",
    "  peak_value=1.0,\n",
    "  warmup_steps=50,\n",
    "  decay_steps=1_000,\n",
    "  end_value=0.0,\n",
    ")\n",
    "\n",
    "optimizer = optax.chain(\n",
    "  optax.clip(1.0),\n",
    "  optax.adamw(learning_rate=schedule),\n",
    ")\n",
    "\n",
    "params = fit(initial_params, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
